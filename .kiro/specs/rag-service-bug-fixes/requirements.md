# Requirements Document

## Introduction

This feature addresses bugs and issues identified in the RAG (Retrieval-Augmented Generation) service through automated testing. The RAG service is a Python FastAPI microservice that processes PDF documents, generates embeddings using Ollama, stores them in ChromaDB, and provides AI-powered question answering capabilities. Six test failures have been identified that indicate bugs in the implementation that need to be fixed to ensure the service operates correctly and reliably.

## Glossary

- **RAG Service**: Python FastAPI microservice using Retrieval-Augmented Generation with Ollama
- **Chunking**: Process of splitting text into overlapping segments for embedding
- **Embedding**: Vector representation of text generated by Ollama
- **ChromaDB**: Vector database for storing document embeddings
- **Ollama**: Local LLM runtime for running models
- **Mock**: Test double that simulates external dependencies
- **Test Failure**: Automated test that does not pass, indicating a bug or issue

## Requirements

### Requirement 1

**User Story:** As a developer, I want the text chunking logic to correctly create overlapping chunks, so that context is preserved across chunk boundaries.

#### Acceptance Criteria

1. WHEN text is chunked with overlap enabled, THE RAG Service SHALL create multiple chunks when the text length exceeds chunk size
2. WHEN the chunk overlap is 50 tokens and chunk size is 100 tokens, THE RAG Service SHALL create at least 2 chunks for text with 150 tokens
3. THE RAG Service SHALL include overlapping content from the previous chunk in each subsequent chunk
4. THE RAG Service SHALL correctly calculate chunk boundaries to ensure proper overlap
5. THE RAG Service SHALL pass the test_create_chunks_overlap test case

### Requirement 2

**User Story:** As a developer, I want PDF download errors to be properly formatted and propagated, so that error handling works correctly throughout the system.

#### Acceptance Criteria

1. WHEN a PDF download fails after retries, THE RAG Service SHALL raise a PDFProcessingError with message starting with "Failed to download PDF"
2. THE RAG Service SHALL include the number of retry attempts in the error message
3. THE RAG Service SHALL include the underlying error details in the error message
4. THE RAG Service SHALL ensure error messages match the expected format for proper error handling
5. THE RAG Service SHALL pass the test_pipeline_error_handling_pdf_download test case

### Requirement 3

**User Story:** As a developer, I want query pipeline tests to properly mock embedding generation, so that integration tests can verify the query flow without external dependencies.

#### Acceptance Criteria

1. WHEN the embedding service is mocked in tests, THE mock SHALL return a list of floats representing the embedding vector
2. THE mocked embedding SHALL have the correct dimension (768 for nomic-embed-text:latest model)
3. THE mock SHALL not return a MagicMock object when get() is called on the embedding response
4. THE RAG Service SHALL correctly handle mocked embeddings in the query pipeline
5. THE RAG Service SHALL pass the test_full_query_pipeline test case

### Requirement 4

**User Story:** As a developer, I want query error handling tests to properly verify exception raising, so that error conditions are correctly tested.

#### Acceptance Criteria

1. WHEN embedding generation fails during query processing, THE RAG Service SHALL raise an appropriate exception
2. THE test SHALL properly mock the embedding service to simulate failure conditions
3. THE test SHALL verify that the exception is raised and propagated correctly
4. THE RAG Service SHALL handle embedding failures gracefully with proper error messages
5. THE RAG Service SHALL pass the test_query_error_handling_embedding_failure test case

### Requirement 5

**User Story:** As a developer, I want LLM service initialization tests to accept model names with version tags, so that tests work with real Ollama model naming conventions.

#### Acceptance Criteria

1. WHEN the LLM service is initialized with default settings, THE test SHALL accept model names with version tags (e.g., "llama3.2:1b")
2. THE test SHALL check for the base model name without requiring exact match to version tag
3. THE RAG Service SHALL support both short model names ("llama3.1") and full names with tags ("llama3.2:1b")
4. THE test SHALL validate that the model name contains the expected base model identifier
5. THE RAG Service SHALL pass the test_initialization_default test case

### Requirement 6

**User Story:** As a developer, I want vector store initialization to handle connection failures gracefully, so that the service can report unhealthy status without crashing.

#### Acceptance Criteria

1. WHEN ChromaDB connection fails during VectorStore initialization, THE RAG Service SHALL handle the error gracefully
2. THE VectorStore SHALL not raise an exception during initialization if connection check fails
3. THE VectorStore SHALL allow the health check endpoint to report degraded status
4. THE VectorStore initialization SHALL defer connection validation to the check_connection method
5. THE RAG Service SHALL pass the test_check_connection_failure test case

### Requirement 7

**User Story:** As a developer, I want the vector store relevance score calculation to handle all distance values correctly, so that queries don't fail with validation errors.

#### Acceptance Criteria

1. WHEN ChromaDB returns distance values, THE RAG Service SHALL normalize them to produce relevance scores between 0.0 and 1.0
2. WHEN distance values are negative or unexpectedly large, THE RAG Service SHALL clamp the relevance score to the valid range [0.0, 1.0]
3. THE RAG Service SHALL not produce negative relevance scores that violate Pydantic validation constraints
4. WHEN relevance scores are calculated, THE RAG Service SHALL use absolute values for distance normalization
5. THE RAG Service SHALL handle edge cases where max_distance equals zero or is negative

### Requirement 8

**User Story:** As a developer, I want all RAG service tests to pass, so that I can be confident the service is working correctly and reliably.

#### Acceptance Criteria

1. THE RAG Service SHALL pass all 103 automated tests without failures
2. THE RAG Service SHALL handle edge cases and error conditions correctly
3. THE RAG Service SHALL maintain backward compatibility with existing functionality
4. THE RAG Service SHALL not introduce new bugs while fixing existing ones
5. THE RAG Service SHALL have test coverage for all critical functionality
